{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport math\nimport time\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport logging\nfrom datetime import datetime\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom os.path import join, exists\nfrom torch.nn import CrossEntropyLoss\nfrom tqdm import tqdm\nfrom torch.nn import DataParallel\nimport transformers\nimport pickle\nimport sys\nfrom sklearn.model_selection import train_test_split\nfrom transformers import GPT2TokenizerFast, GPT2LMHeadModel, GPT2Config\nfrom transformers import BertTokenizerFast\nimport pandas as pd\nimport torch.nn.utils.rnn as rnn_utils\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-12-20T07:37:49.684219Z","iopub.execute_input":"2022-12-20T07:37:49.684568Z","iopub.status.idle":"2022-12-20T07:37:49.691637Z","shell.execute_reply.started":"2022-12-20T07:37:49.684540Z","shell.execute_reply":"2022-12-20T07:37:49.690381Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## 全部配置","metadata":{}},{"cell_type":"code","source":"class MyConfig():\n    def __init__(self):\n        self.device = '0'    #设置使用哪些显卡\n        self.no_cuda = False    #不使用GPU进行训练\n        self.vocab_path = '/kaggle/input/paworks/vocab.txt'    #词表路径\n        self.model_config = '/kaggle/input/paworks/config.json'    #设置模型参数\n        self.train_path = '/kaggle/input/paworks/train.pkl'    #训练集路径\n        self.max_len = 150    #训练时，输入数据的最大长度\n        self.log_path = 'train.log'    #训练日志存放位置\n        self.log = True    #是否记录日志\n        self.ignore_index = -100    #对于ignore_index的label token不计算梯度\n        self.epochs = 40    #训练的最大轮次\n        self.batch_size = 32    #训练的batch size\n        self.gpu0_bsz = 32    #0号卡的batch size\n        self.lr = 2.6e-5    #学习率\n        self.eps = 1.0e-09    #衰减率\n        self.log_step = 1    #多少步汇报一次loss\n        self.gradient_accumulation_steps = 4    #梯度积累\n        self.max_grad_norm = 2.0    #参数的梯度范数\n        self.save_model_path = 'model'    #模型输出路径\n        self.pretrained_model = '/kaggle/input/paworks/pytorch_model.bin'    #预训练的模型的路径\n        self.num_workers = 2    #dataloader加载数据时使用的线程数量\n        self.patience = 0    #用于early stopping,设为0时,不进行early stopping.early stop得到的模型的生成效果不一定会更好。\n        self.warmup_steps = 4000    #warm up步数\n        self.val_num = 1000    #验证集大小","metadata":{"execution":{"iopub.status.busy":"2022-12-20T07:37:51.346437Z","iopub.execute_input":"2022-12-20T07:37:51.346801Z","iopub.status.idle":"2022-12-20T07:37:51.355034Z","shell.execute_reply.started":"2022-12-20T07:37:51.346771Z","shell.execute_reply":"2022-12-20T07:37:51.353965Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"my_config = MyConfig()\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = my_config.device\n\nmy_config.cuda = not my_config.no_cuda\n\nif my_config.batch_size < 2048 and my_config.warmup_steps <= 4000:\n    print('[Warning] The warmup steps may be not enough.\\n' \\\n          '(sz_b, warmup) = (2048, 4000) is the official setting.\\n' \\\n          'Using smaller batch w/o longer warmup may cause ' \\\n          'the warmup stage ends with only little data trained.')\n\n# 当用户使用GPU,并且GPU可用时\nmy_config.cuda = torch.cuda.is_available() and not my_config.no_cuda\ndevice = 'cuda:0' if my_config.cuda else 'cpu'\nmy_config.device = device\nprint('using device:{} config:{}'.format(device,my_config.device))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T07:37:39.763200Z","iopub.execute_input":"2022-12-20T07:37:39.763557Z","iopub.status.idle":"2022-12-20T07:37:39.773886Z","shell.execute_reply.started":"2022-12-20T07:37:39.763527Z","shell.execute_reply":"2022-12-20T07:37:39.772909Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[Warning] The warmup steps may be not enough.\n(sz_b, warmup) = (2048, 4000) is the official setting.\nUsing smaller batch w/o longer warmup may cause the warmup stage ends with only little data trained.\nusing device:cuda:0 config:cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 代码主体","metadata":{}},{"cell_type":"markdown","source":"## class MyDataset(Dataset):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self, input_list, max_len):\n        self.input_list = input_list\n        self.max_len = max_len\n\n    def __getitem__(self, index):\n        input_ids = self.input_list[index]\n        input_ids = input_ids[:self.max_len]\n        input_ids = torch.tensor(input_ids, dtype=torch.long)\n        return input_ids\n\n    def __len__(self):\n        return len(self.input_list)\n\n\ndef run():\n    my_config = MyConfig()\n\n    # 设置使用哪些显卡进行训练\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = my_config.device\n\n    my_config.cuda = not my_config.no_cuda\n\n    if my_config.batch_size < 2048 and my_config.warmup_steps <= 4000:\n        print('[Warning] The warmup steps may be not enough.\\n' \\\n              '(sz_b, warmup) = (2048, 4000) is the official setting.\\n' \\\n              'Using smaller batch w/o longer warmup may cause ' \\\n              'the warmup stage ends with only little data trained.')\n\n    # 创建日志对象\n    logger = create_logger(my_config)\n    # 当用户使用GPU,并且GPU可用时\n    my_config.cuda = torch.cuda.is_available() and not my_config.no_cuda\n    device = 'cuda:0' if my_config.cuda else 'cpu'\n    my_config.device = device\n    print('using device:{}'.format(device))\n\n    # 初始化tokenizer\n    tokenizer = BertTokenizerFast(vocab_file=my_config.vocab_path, sep_token=\"[SEP]\", pad_token=\"[PAD]\",\n                                  cls_token=\"[CLS]\")\n    my_config.sep_id = tokenizer.sep_token_id\n    my_config.pad_id = tokenizer.pad_token_id\n    my_config.cls_id = tokenizer.cls_token_id\n\n    # 创建模型的输出目录\n    if not os.path.exists(my_config.save_model_path):\n        os.mkdir(my_config.save_model_path)\n\n    # 创建模型\n    if my_config.pretrained_model:  # 加载预训练模型\n        print(f'model_config_path:{my_config.model_config}  model:{my_config.pretrained_model}')\n        model_config = GPT2Config.from_json_file(my_config.model_config)\n        model = GPT2LMHeadModel.from_pretrained(my_config.pretrained_model, config=model_config)\n    else:  # 初始化模型\n        model_config = GPT2Config.from_json_file(my_config.model_config)\n        model = GPT2LMHeadModel(config=model_config)\n    model = model.to(device)\n    print('model config:\\n{}'.format(model.config.to_json_string()))\n    assert model.config.vocab_size == tokenizer.vocab_size\n\n    # 并行训练模型\n    if my_config.cuda and torch.cuda.device_count() > 1:\n        model = DataParallel(model).cuda()\n        # model = BalancedDataParallel(my_config.gpu0_bsz, model, dim=0).cuda()\n        print(\"use GPU {} to train\".format(my_config.device))\n\n    # 计算模型参数数量\n    num_parameters = 0\n    parameters = model.parameters()\n    for parameter in parameters:\n        num_parameters += parameter.numel()\n    print('number of model parameters: {}'.format(num_parameters))\n\n    # 记录参数设置\n    print(\"my_config:{}\".format(my_config))\n\n    # 加载训练集和验证集\n    # ========= Loading Dataset ========= #\n    train_dataset, validate_dataset = load_dataset(logger, my_config)\n\n    train(model, logger, train_dataset, validate_dataset, my_config)\n\n\ndef create_logger(my_config):\n    \"\"\"\n    将日志输出到日志文件和控制台\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n\n    formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(message)s')\n\n    # 创建一个handler，用于写入日志文件\n    file_handler = logging.FileHandler(\n        filename=my_config.log_path)\n    file_handler.setFormatter(formatter)\n    file_handler.setLevel(logging.INFO)\n    logger.addHandler(file_handler)\n\n    # 创建一个handler，用于将日志输出到控制台\n    console = logging.StreamHandler()\n    console.setLevel(logging.DEBUG)\n    console.setFormatter(formatter)\n    logger.addHandler(console)\n\n    return logger\n\n\ndef load_dataset(logger, my_config):\n    \"\"\"\n    加载训练集和验证集\n    \"\"\"\n    print(\"loading training dataset and validating dataset\")\n    train_path = my_config.train_path\n\n    with open(train_path, \"rb\") as f:\n        input_list = pickle.load(f)\n\n    # 划分训练集与验证集\n    val_num = my_config.val_num\n    input_list_train = input_list[val_num:]\n    input_list_val = input_list[:val_num]\n    # test\n    # input_list_train = input_list_train[:24]\n    # input_list_val = input_list_val[:24]\n    print(f'train {len(input_list_train)}')\n    print(f'valid {len(input_list_val)}')\n    train_dataset = MyDataset(input_list_train, my_config.max_len)\n    val_dataset = MyDataset(input_list_val, my_config.max_len)\n\n    return train_dataset, val_dataset\n\n\ndef train_epoch(model, train_dataloader, optimizer, scheduler, logger,\n                epoch, my_config):\n    model.train()\n    device = my_config.device\n    # pad_id = my_config.pad_id\n    # sep_id = my_config.sep_id\n    ignore_index = my_config.ignore_index\n    epoch_start_time = datetime.now()\n    total_loss = 0  # 记录下整个epoch的loss的总和\n\n    # epoch_correct_num:每个epoch中,output预测正确的word的数量\n    # epoch_total_num: 每个epoch中,output预测的word的总数量\n    epoch_correct_num, epoch_total_num = 0, 0\n\n    for batch_idx, (input_ids, labels) in enumerate(train_dataloader):\n        # 捕获cuda out of memory exception\n        try:\n\n            input_ids = input_ids.to(device)\n            labels = labels.to(device)\n            outputs = model.forward(input_ids, labels=labels)\n            logits = outputs.logits\n            loss = outputs.loss\n            loss = loss.mean()\n\n            # 统计该batch的预测token的正确数与总数\n            batch_correct_num, batch_total_num = calculate_acc(logits, labels, ignore_index=ignore_index)\n            # 统计该epoch的预测token的正确数与总数\n            epoch_correct_num += batch_correct_num\n            epoch_total_num += batch_total_num\n            # 计算该batch的accuracy\n            batch_acc = batch_correct_num / batch_total_num\n\n            total_loss += loss.item()\n            if my_config.gradient_accumulation_steps > 1:\n                loss = loss / my_config.gradient_accumulation_steps\n\n            loss.backward()\n            # 梯度裁剪\n            torch.nn.utils.clip_grad_norm_(model.parameters(), my_config.max_grad_norm)\n\n            # 进行一定step的梯度累计之后，更新参数\n            if (batch_idx + 1) % my_config.gradient_accumulation_steps == 0:\n                # 更新参数\n                optimizer.step()\n                # 更新学习率\n                scheduler.step()\n                # 清空梯度信息\n                optimizer.zero_grad()\n\n            if (batch_idx + 1) % my_config.log_step == 0:\n                print(\n                    \"batch {} of epoch {}, loss {}, batch_acc {}, lr {}\".format(\n                        batch_idx + 1, epoch + 1, loss.item() * my_config.gradient_accumulation_steps, batch_acc,\n                        scheduler.get_lr()))\n\n            del input_ids, outputs\n\n        except RuntimeError as exception:\n            if \"out of memory\" in str(exception):\n                print(\"WARNING: ran out of memory\")\n                if hasattr(torch.cuda, 'empty_cache'):\n                    torch.cuda.empty_cache()\n            else:\n                print(str(exception))\n                raise exception\n\n    # 记录当前epoch的平均loss与accuracy\n    epoch_mean_loss = total_loss / len(train_dataloader)\n    epoch_mean_acc = epoch_correct_num / epoch_total_num\n    print(\n        \"epoch {}: loss {}, predict_acc {} \".format(epoch + 1, epoch_mean_loss, epoch_mean_acc))\n\n    # save model\n    print('saving model for epoch {}'.format(epoch + 1))\n    model_path = join(my_config.save_model_path, 'epoch{}'.format(epoch + 1))\n    if not os.path.exists(model_path):\n        os.mkdir(model_path)\n    model_to_save = model.module if hasattr(model, 'module') else model\n    model_to_save.save_pretrained(model_path)\n    print('epoch {} finished'.format(epoch + 1))\n    epoch_finish_time = datetime.now()\n    print('time for one epoch: {}'.format(epoch_finish_time - epoch_start_time))\n\n    return epoch_mean_loss\n\n\ndef validate_epoch(model, validate_dataloader, logger, epoch, my_config):\n    print(f\"start validating {len(validate_dataloader)}\")\n    model.eval()\n    device = my_config.device\n    # pad_id = my_config.pad_id\n    # sep_id = my_config.sep_id\n    ignore_index = my_config.ignore_index\n    epoch_start_time = datetime.now()\n    total_loss = 0\n    # 捕获cuda out of memory exception\n    try:\n        with torch.no_grad():\n            for batch_idx, (input_ids, labels) in enumerate(validate_dataloader):\n                input_ids = input_ids.to(device)\n                labels = labels.to(device)\n                outputs = model.forward(input_ids, labels=labels)\n                logits = outputs.logits\n                loss = outputs.loss\n                loss = loss.mean()\n\n                total_loss += loss.item()\n                del input_ids, outputs\n\n            # 记录当前epoch的平均loss\n            epoch_mean_loss = total_loss / len(validate_dataloader)\n            print(\n                \"validate epoch {}: loss {}\".format(epoch + 1, epoch_mean_loss))\n            epoch_finish_time = datetime.now()\n            print('time for validating one epoch: {}'.format(epoch_finish_time - epoch_start_time))\n            return epoch_mean_loss\n    except RuntimeError as exception:\n        if \"out of memory\" in str(exception):\n            print(\"WARNING: ran out of memory\")\n            if hasattr(torch.cuda, 'empty_cache'):\n                torch.cuda.empty_cache()\n        else:\n            print(str(exception))\n            raise exception\n\n\ndef train(model, logger, train_dataset, validate_dataset, my_config):\n    train_dataloader = DataLoader(\n        train_dataset, batch_size=my_config.batch_size, shuffle=True, num_workers=my_config.num_workers,\n        collate_fn=collate_fn,\n        drop_last=True\n    )\n    validate_dataloader = DataLoader(validate_dataset, batch_size=my_config.batch_size, shuffle=True,\n                                     num_workers=my_config.num_workers, collate_fn=collate_fn, drop_last=True)\n    #     early_stopping = EarlyStopping(my_config.patience, verbose=True, save_path=my_config.save_model_path)\n    t_total = len(train_dataloader) // my_config.gradient_accumulation_steps * my_config.epochs\n    optimizer = transformers.AdamW(model.parameters(), lr=my_config.lr, eps=my_config.eps)\n    # scheduler = transformers.WarmupLinearSchedule(optimizer, warmup_steps=my_config.warmup_steps, t_total=t_total)\n    scheduler = transformers.get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=my_config.warmup_steps, num_training_steps=t_total\n    )\n\n    print('starting training')\n\n    # 用于记录每个epoch训练和验证的loss\n    train_losses, validate_losses = [], []\n    # 记录验证集的最小loss\n    best_val_loss = 10000\n    # 开始训练\n    for epoch in range(my_config.epochs):\n        # ========== train ========== #\n        train_loss = train_epoch(\n            model=model, train_dataloader=train_dataloader,\n            optimizer=optimizer, scheduler=scheduler,\n            logger=logger, epoch=epoch, my_config=my_config)\n        train_losses.append(train_loss)\n\n        # ========== validate ========== #\n        validate_loss = validate_epoch(\n            model=model, validate_dataloader=validate_dataloader,\n            logger=logger, epoch=epoch, my_config=my_config)\n        validate_losses.append(validate_loss)\n\n        # 保存当前困惑度最低的模型，困惑度低，模型的生成效果不一定会越好\n        if validate_loss < best_val_loss:\n            best_val_loss = validate_loss\n            print('saving current best model for epoch {}'.format(epoch + 1))\n            model_path = join(my_config.save_model_path, 'min_ppl_model'.format(epoch + 1))\n            if not os.path.exists(model_path):\n                os.mkdir(model_path)\n            model_to_save = model.module if hasattr(model, 'module') else model\n            model_to_save.save_pretrained(model_path)\n\n        #  如果patience=0,则不进行early stopping\n        if my_config.patience == 0:\n            continue\n        # 这里不知道为何无法导入EarlyStopping\n    #         early_stopping(validate_loss, model)\n    #         if early_stopping.early_stop:\n    #             print(\"Early stopping\")\n    #             break\n    print('training finished')\n    print(\"train_losses:{}\".format(train_losses))\n    print(\"validate_losses:{}\".format(validate_losses))\n\n\ndef caculate_loss(logit, target, pad_idx, smoothing=True):\n    if smoothing:\n        logit = logit[..., :-1, :].contiguous().view(-1, logit.size(2))\n        target = target[..., 1:].contiguous().view(-1)\n\n        eps = 0.1\n        n_class = logit.size(-1)\n\n        one_hot = torch.zeros_like(logit).scatter(1, target.view(-1, 1), 1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n        log_prb = F.log_softmax(logit, dim=1)\n\n        non_pad_mask = target.ne(pad_idx)\n        loss = -(one_hot * log_prb).sum(dim=1)\n        loss = loss.masked_select(non_pad_mask).mean()  # average later\n    else:\n        # loss = F.cross_entropy(predict_logit, target, ignore_index=pad_idx)\n        logit = logit[..., :-1, :].contiguous().view(-1, logit.size(-1))\n        labels = target[..., 1:].contiguous().view(-1)\n        loss = F.cross_entropy(logit, labels, ignore_index=pad_idx)\n    return loss\n\n\ndef calculate_acc(logit, labels, ignore_index=-100):\n    logit = logit[..., :-1, :].contiguous().view(-1, logit.size(-1))\n    labels = labels[..., 1:].contiguous().view(-1)\n\n    _, logit = logit.max(dim=-1)  # 对于每条数据，返回最大的index\n    # 进行非运算，返回一个tensor，若labels的第i个位置为pad_id，则置为0，否则为1\n    non_pad_mask = labels.ne(ignore_index)\n    n_correct = logit.eq(labels).masked_select(non_pad_mask).sum().item()\n    n_word = non_pad_mask.sum().item()\n    return n_correct, n_word\n\n\ndef collate_fn(batch):\n    input_ids = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=0)\n    labels = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=-100)\n    return input_ids, labels\n\n\nrun()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T07:37:54.826789Z","iopub.execute_input":"2022-12-20T07:37:54.827230Z"}}}]}