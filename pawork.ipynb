{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport math\nimport time\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport logging\nfrom datetime import datetime\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom os.path import join, exists\nfrom torch.nn import CrossEntropyLoss\nfrom tqdm import tqdm\nfrom torch.nn import DataParallel\nimport transformers\nimport pickle\nimport sys\nfrom sklearn.model_selection import train_test_split\nfrom transformers import GPT2TokenizerFast, GPT2LMHeadModel, GPT2Config\nfrom transformers import BertTokenizerFast\nimport pandas as pd\nimport torch.nn.utils.rnn as rnn_utils\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-12-21T08:23:37.597252Z","iopub.execute_input":"2022-12-21T08:23:37.597862Z","iopub.status.idle":"2022-12-21T08:23:43.101134Z","shell.execute_reply.started":"2022-12-21T08:23:37.597733Z","shell.execute_reply":"2022-12-21T08:23:43.099388Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 全部配置","metadata":{}},{"cell_type":"code","source":"class MyConfig():\n    def __init__(self):\n        self.device = '0'    #设置使用哪些显卡\n        self.no_cuda = False    #不使用GPU进行训练\n        self.vocab_path = '/kaggle/input/paworks/vocab.txt'    #词表路径\n        self.model_config = '/kaggle/input/paworks/config.json'    #设置模型参数\n        self.train_path = '/kaggle/input/paworks/train-v3.pkl'    #训练集路径\n        self.max_len = 150    #训练时，输入数据的最大长度\n        self.log_path = 'train.log'    #训练日志存放位置\n        self.log = True    #是否记录日志\n        self.ignore_index = -100    #对于ignore_index的label token不计算梯度\n        self.epochs = 100    #训练的最大轮次\n        self.batch_size = 32    #训练的batch size\n        self.gpu0_bsz = 32    #0号卡的batch size\n        self.lr = 2.6e-5    #学习率\n        self.eps = 1.0e-09    #衰减率\n        self.log_step = 1    #多少步汇报一次loss\n        self.gradient_accumulation_steps = 4    #梯度积累\n        self.max_grad_norm = 2.0    #参数的梯度范数\n        self.save_model_path = 'model'    #模型输出路径\n        self.pretrained_model = '/kaggle/input/paworks/pytorch_model-e63.bin'    #预训练的模型的路径\n        self.num_workers = 2    #dataloader加载数据时使用的线程数量\n        self.patience = 0    #用于early stopping,设为0时,不进行early stopping.early stop得到的模型的生成效果不一定会更好。\n        self.warmup_steps = 4000    #warm up步数\n        self.val_num = 1000    #验证集大小","metadata":{"execution":{"iopub.status.busy":"2022-12-21T08:23:43.106841Z","iopub.execute_input":"2022-12-21T08:23:43.109564Z","iopub.status.idle":"2022-12-21T08:23:43.121055Z","shell.execute_reply.started":"2022-12-21T08:23:43.109525Z","shell.execute_reply":"2022-12-21T08:23:43.119644Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"my_config = MyConfig()\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = my_config.device\n\nmy_config.cuda = not my_config.no_cuda\n\nif my_config.batch_size < 2048 and my_config.warmup_steps <= 4000:\n    print('[Warning] The warmup steps may be not enough.\\n' \\\n          '(sz_b, warmup) = (2048, 4000) is the official setting.\\n' \\\n          'Using smaller batch w/o longer warmup may cause ' \\\n          'the warmup stage ends with only little data trained.')\n\n# 当用户使用GPU,并且GPU可用时\nmy_config.cuda = torch.cuda.is_available() and not my_config.no_cuda\ndevice = 'cuda:0' if my_config.cuda else 'cpu'\nmy_config.device = device\nprint('using device:{} config:{}'.format(device,my_config.device))","metadata":{"execution":{"iopub.status.busy":"2022-12-21T08:23:43.124664Z","iopub.execute_input":"2022-12-21T08:23:43.127262Z","iopub.status.idle":"2022-12-21T08:23:43.219757Z","shell.execute_reply.started":"2022-12-21T08:23:43.127217Z","shell.execute_reply":"2022-12-21T08:23:43.217066Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[Warning] The warmup steps may be not enough.\n(sz_b, warmup) = (2048, 4000) is the official setting.\nUsing smaller batch w/o longer warmup may cause the warmup stage ends with only little data trained.\nusing device:cuda:0 config:cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 代码主体","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self, input_list, max_len):\n        self.input_list = input_list\n        self.max_len = max_len\n\n    def __getitem__(self, index):\n        input_ids = self.input_list[index]\n        input_ids = input_ids[:self.max_len]\n        input_ids = torch.tensor(input_ids, dtype=torch.long)\n        return input_ids\n\n    def __len__(self):\n        return len(self.input_list)\n\n\ndef run():\n    my_config = MyConfig()\n\n    # 设置使用哪些显卡进行训练\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = my_config.device\n\n    my_config.cuda = not my_config.no_cuda\n\n    if my_config.batch_size < 2048 and my_config.warmup_steps <= 4000:\n        print('[Warning] The warmup steps may be not enough.\\n' \\\n              '(sz_b, warmup) = (2048, 4000) is the official setting.\\n' \\\n              'Using smaller batch w/o longer warmup may cause ' \\\n              'the warmup stage ends with only little data trained.')\n\n    # 创建日志对象\n    logger = create_logger(my_config)\n    # 当用户使用GPU,并且GPU可用时\n    my_config.cuda = torch.cuda.is_available() and not my_config.no_cuda\n    device = 'cuda:0' if my_config.cuda else 'cpu'\n    my_config.device = device\n    print('using device:{}'.format(device))\n\n    # 初始化tokenizer\n    tokenizer = BertTokenizerFast(vocab_file=my_config.vocab_path, sep_token=\"[SEP]\", pad_token=\"[PAD]\",\n                                  cls_token=\"[CLS]\")\n    my_config.sep_id = tokenizer.sep_token_id\n    my_config.pad_id = tokenizer.pad_token_id\n    my_config.cls_id = tokenizer.cls_token_id\n\n    # 创建模型的输出目录\n    if not os.path.exists(my_config.save_model_path):\n        os.mkdir(my_config.save_model_path)\n\n    # 创建模型\n    if my_config.pretrained_model:  # 加载预训练模型\n        print(f'model_config_path:{my_config.model_config}  model:{my_config.pretrained_model}')\n        model_config = GPT2Config.from_json_file(my_config.model_config)\n        model = GPT2LMHeadModel.from_pretrained(my_config.pretrained_model, config=model_config)\n    else:  # 初始化模型\n        model_config = GPT2Config.from_json_file(my_config.model_config)\n        model = GPT2LMHeadModel(config=model_config)\n    model = model.to(device)\n    print('model config:\\n{}'.format(model.config.to_json_string()))\n    assert model.config.vocab_size == tokenizer.vocab_size\n\n    # 并行训练模型\n    if my_config.cuda and torch.cuda.device_count() > 1:\n        model = DataParallel(model).cuda()\n        # model = BalancedDataParallel(my_config.gpu0_bsz, model, dim=0).cuda()\n        print(\"use GPU {} to train\".format(my_config.device))\n\n    # 计算模型参数数量\n    num_parameters = 0\n    parameters = model.parameters()\n    for parameter in parameters:\n        num_parameters += parameter.numel()\n    print('number of model parameters: {}'.format(num_parameters))\n\n    # 记录参数设置\n    print(\"my_config:{}\".format(my_config))\n\n    # 加载训练集和验证集\n    # ========= Loading Dataset ========= #\n    train_dataset, validate_dataset = load_dataset(logger, my_config)\n\n    train(model, logger, train_dataset, validate_dataset, my_config)\n\n\ndef create_logger(my_config):\n    \"\"\"\n    将日志输出到日志文件和控制台\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n\n    formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(message)s')\n\n    # 创建一个handler，用于写入日志文件\n    file_handler = logging.FileHandler(\n        filename=my_config.log_path)\n    file_handler.setFormatter(formatter)\n    file_handler.setLevel(logging.INFO)\n    logger.addHandler(file_handler)\n\n    # 创建一个handler，用于将日志输出到控制台\n    console = logging.StreamHandler()\n    console.setLevel(logging.DEBUG)\n    console.setFormatter(formatter)\n    logger.addHandler(console)\n\n    return logger\n\n\ndef load_dataset(logger, my_config):\n    \"\"\"\n    加载训练集和验证集\n    \"\"\"\n    print(\"loading training dataset and validating dataset\")\n    train_path = my_config.train_path\n\n    with open(train_path, \"rb\") as f:\n        input_list = pickle.load(f)\n\n    # 划分训练集与验证集\n    val_num = my_config.val_num\n    input_list_train = input_list[val_num:]\n    input_list_val = input_list[:val_num]\n    # test\n    # input_list_train = input_list_train[:24]\n    # input_list_val = input_list_val[:24]\n    print(f'train {len(input_list_train)}')\n    print(f'valid {len(input_list_val)}')\n    train_dataset = MyDataset(input_list_train, my_config.max_len)\n    val_dataset = MyDataset(input_list_val, my_config.max_len)\n\n    return train_dataset, val_dataset\n\n\ndef train_epoch(model, train_dataloader, optimizer, scheduler, logger,\n                epoch, my_config):\n    model.train()\n    device = my_config.device\n    # pad_id = my_config.pad_id\n    # sep_id = my_config.sep_id\n    ignore_index = my_config.ignore_index\n    epoch_start_time = datetime.now()\n    total_loss = 0  # 记录下整个epoch的loss的总和\n\n    # epoch_correct_num:每个epoch中,output预测正确的word的数量\n    # epoch_total_num: 每个epoch中,output预测的word的总数量\n    epoch_correct_num, epoch_total_num = 0, 0\n\n    for batch_idx, (input_ids, labels) in enumerate(train_dataloader):\n        # 捕获cuda out of memory exception\n        try:\n\n            input_ids = input_ids.to(device)\n            labels = labels.to(device)\n            outputs = model.forward(input_ids, labels=labels)\n            logits = outputs.logits\n            loss = outputs.loss\n            loss = loss.mean()\n\n            # 统计该batch的预测token的正确数与总数\n            batch_correct_num, batch_total_num = calculate_acc(logits, labels, ignore_index=ignore_index)\n            # 统计该epoch的预测token的正确数与总数\n            epoch_correct_num += batch_correct_num\n            epoch_total_num += batch_total_num\n            # 计算该batch的accuracy\n            batch_acc = batch_correct_num / batch_total_num\n\n            total_loss += loss.item()\n            if my_config.gradient_accumulation_steps > 1:\n                loss = loss / my_config.gradient_accumulation_steps\n\n            loss.backward()\n            # 梯度裁剪\n            torch.nn.utils.clip_grad_norm_(model.parameters(), my_config.max_grad_norm)\n\n            # 进行一定step的梯度累计之后，更新参数\n            if (batch_idx + 1) % my_config.gradient_accumulation_steps == 0:\n                # 更新参数\n                optimizer.step()\n                # 更新学习率\n                scheduler.step()\n                # 清空梯度信息\n                optimizer.zero_grad()\n\n            if (batch_idx + 1) % my_config.log_step == 0:\n                print(\n                    \"batch {} of epoch {}, loss {}, batch_acc {}, lr {}\".format(\n                        batch_idx + 1, epoch + 1, loss.item() * my_config.gradient_accumulation_steps, batch_acc,\n                        scheduler.get_lr()))\n\n            del input_ids, outputs\n\n        except RuntimeError as exception:\n            if \"out of memory\" in str(exception):\n                print(\"WARNING: ran out of memory\")\n                if hasattr(torch.cuda, 'empty_cache'):\n                    torch.cuda.empty_cache()\n            else:\n                print(str(exception))\n                raise exception\n\n    # 记录当前epoch的平均loss与accuracy\n    epoch_mean_loss = total_loss / len(train_dataloader)\n    epoch_mean_acc = epoch_correct_num / epoch_total_num\n    print(\n        \"epoch {}: loss {}, predict_acc {} \".format(epoch + 1, epoch_mean_loss, epoch_mean_acc))\n\n    # save model\n    print('saving model for epoch {}'.format(epoch + 1))\n    model_path = join(my_config.save_model_path, 'epoch{}'.format(epoch + 1))\n    if not os.path.exists(model_path):\n        os.mkdir(model_path)\n    model_to_save = model.module if hasattr(model, 'module') else model\n    model_to_save.save_pretrained(model_path)\n    print('epoch {} finished'.format(epoch + 1))\n    epoch_finish_time = datetime.now()\n    print('time for one epoch: {}'.format(epoch_finish_time - epoch_start_time))\n\n    return epoch_mean_loss\n\n\ndef validate_epoch(model, validate_dataloader, logger, epoch, my_config):\n    print(f\"start validating {len(validate_dataloader)}\")\n    model.eval()\n    device = my_config.device\n    # pad_id = my_config.pad_id\n    # sep_id = my_config.sep_id\n    ignore_index = my_config.ignore_index\n    epoch_start_time = datetime.now()\n    total_loss = 0\n    # 捕获cuda out of memory exception\n    try:\n        with torch.no_grad():\n            for batch_idx, (input_ids, labels) in enumerate(validate_dataloader):\n                input_ids = input_ids.to(device)\n                labels = labels.to(device)\n                outputs = model.forward(input_ids, labels=labels)\n                logits = outputs.logits\n                loss = outputs.loss\n                loss = loss.mean()\n\n                total_loss += loss.item()\n                del input_ids, outputs\n\n            # 记录当前epoch的平均loss\n            epoch_mean_loss = total_loss / len(validate_dataloader)\n            print(\n                \"validate epoch {}: loss {}\".format(epoch + 1, epoch_mean_loss))\n            epoch_finish_time = datetime.now()\n            print('time for validating one epoch: {}'.format(epoch_finish_time - epoch_start_time))\n            return epoch_mean_loss\n    except RuntimeError as exception:\n        if \"out of memory\" in str(exception):\n            print(\"WARNING: ran out of memory\")\n            if hasattr(torch.cuda, 'empty_cache'):\n                torch.cuda.empty_cache()\n        else:\n            print(str(exception))\n            raise exception\n\n\ndef train(model, logger, train_dataset, validate_dataset, my_config):\n    train_dataloader = DataLoader(\n        train_dataset, batch_size=my_config.batch_size, shuffle=True, num_workers=my_config.num_workers,\n        collate_fn=collate_fn,\n        drop_last=True\n    )\n    validate_dataloader = DataLoader(validate_dataset, batch_size=my_config.batch_size, shuffle=True,\n                                     num_workers=my_config.num_workers, collate_fn=collate_fn, drop_last=True)\n    #     early_stopping = EarlyStopping(my_config.patience, verbose=True, save_path=my_config.save_model_path)\n    t_total = len(train_dataloader) // my_config.gradient_accumulation_steps * my_config.epochs\n    optimizer = transformers.AdamW(model.parameters(), lr=my_config.lr, eps=my_config.eps)\n    # scheduler = transformers.WarmupLinearSchedule(optimizer, warmup_steps=my_config.warmup_steps, t_total=t_total)\n    scheduler = transformers.get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=my_config.warmup_steps, num_training_steps=t_total\n    )\n\n    print('starting training')\n\n    # 用于记录每个epoch训练和验证的loss\n    train_losses, validate_losses = [], []\n    # 记录验证集的最小loss\n    best_val_loss = 10000\n    # 开始训练\n    for epoch in range(my_config.epochs):\n        # ========== train ========== #\n        train_loss = train_epoch(\n            model=model, train_dataloader=train_dataloader,\n            optimizer=optimizer, scheduler=scheduler,\n            logger=logger, epoch=epoch, my_config=my_config)\n        train_losses.append(train_loss)\n\n        # ========== validate ========== #\n        validate_loss = validate_epoch(\n            model=model, validate_dataloader=validate_dataloader,\n            logger=logger, epoch=epoch, my_config=my_config)\n        validate_losses.append(validate_loss)\n\n        # 保存当前困惑度最低的模型，困惑度低，模型的生成效果不一定会越好\n        if validate_loss < best_val_loss:\n            best_val_loss = validate_loss\n            print('saving current best model for epoch {}'.format(epoch + 1))\n            model_path = join(my_config.save_model_path, 'min_ppl_model'.format(epoch + 1))\n            if not os.path.exists(model_path):\n                os.mkdir(model_path)\n            model_to_save = model.module if hasattr(model, 'module') else model\n            model_to_save.save_pretrained(model_path)\n\n        #  如果patience=0,则不进行early stopping\n        if my_config.patience == 0:\n            continue\n        # 这里不知道为何无法导入EarlyStopping\n    #         early_stopping(validate_loss, model)\n    #         if early_stopping.early_stop:\n    #             print(\"Early stopping\")\n    #             break\n    print('training finished')\n    print(\"train_losses:{}\".format(train_losses))\n    print(\"validate_losses:{}\".format(validate_losses))\n\n\ndef caculate_loss(logit, target, pad_idx, smoothing=True):\n    if smoothing:\n        logit = logit[..., :-1, :].contiguous().view(-1, logit.size(2))\n        target = target[..., 1:].contiguous().view(-1)\n\n        eps = 0.1\n        n_class = logit.size(-1)\n\n        one_hot = torch.zeros_like(logit).scatter(1, target.view(-1, 1), 1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n        log_prb = F.log_softmax(logit, dim=1)\n\n        non_pad_mask = target.ne(pad_idx)\n        loss = -(one_hot * log_prb).sum(dim=1)\n        loss = loss.masked_select(non_pad_mask).mean()  # average later\n    else:\n        # loss = F.cross_entropy(predict_logit, target, ignore_index=pad_idx)\n        logit = logit[..., :-1, :].contiguous().view(-1, logit.size(-1))\n        labels = target[..., 1:].contiguous().view(-1)\n        loss = F.cross_entropy(logit, labels, ignore_index=pad_idx)\n    return loss\n\n\ndef calculate_acc(logit, labels, ignore_index=-100):\n    logit = logit[..., :-1, :].contiguous().view(-1, logit.size(-1))\n    labels = labels[..., 1:].contiguous().view(-1)\n\n    _, logit = logit.max(dim=-1)  # 对于每条数据，返回最大的index\n    # 进行非运算，返回一个tensor，若labels的第i个位置为pad_id，则置为0，否则为1\n    non_pad_mask = labels.ne(ignore_index)\n    n_correct = logit.eq(labels).masked_select(non_pad_mask).sum().item()\n    n_word = non_pad_mask.sum().item()\n    return n_correct, n_word\n\n\ndef collate_fn(batch):\n    input_ids = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=0)\n    labels = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=-100)\n    return input_ids, labels\n\n\nrun()","metadata":{"execution":{"iopub.status.busy":"2022-12-21T08:23:43.226827Z","iopub.execute_input":"2022-12-21T08:23:43.227719Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[Warning] The warmup steps may be not enough.\n(sz_b, warmup) = (2048, 4000) is the official setting.\nUsing smaller batch w/o longer warmup may cause the warmup stage ends with only little data trained.\nusing device:cuda:0\nmodel_config_path:/kaggle/input/paworks/config.json  model:/kaggle/input/paworks/pytorch_model.bin\nmodel config:\n{\n  \"_name_or_path\": \"/kaggle/input/paworks/pytorch_model.bin\",\n  \"activation_function\": \"gelu_new\",\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 300,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 10,\n  \"n_positions\": 300,\n  \"output_past\": true,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 13317\n}\n\nnumber of model parameters: 81338112\nmy_config:<__main__.MyConfig object at 0x7f9f6f4e7210>\nloading training dataset and validating dataset\ntrain 11954\nvalid 1000\nstarting training\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"batch 1 of epoch 1, loss 6.41118049621582, batch_acc 0.20926517571884984, lr [0.0]\nbatch 2 of epoch 1, loss 6.366540431976318, batch_acc 0.20675105485232068, lr [0.0]\nbatch 3 of epoch 1, loss 6.181403636932373, batch_acc 0.2062955254942768, lr [0.0]\nbatch 4 of epoch 1, loss 6.373268127441406, batch_acc 0.20557491289198607, lr [6.4999999999999995e-09]\nbatch 5 of epoch 1, loss 6.3286237716674805, batch_acc 0.20638297872340425, lr [6.4999999999999995e-09]\nbatch 6 of epoch 1, loss 5.919843673706055, batch_acc 0.2298991885910991, lr [6.4999999999999995e-09]\nbatch 7 of epoch 1, loss 6.227814197540283, batch_acc 0.21325107296137338, lr [6.4999999999999995e-09]\nbatch 8 of epoch 1, loss 6.590843200683594, batch_acc 0.1984911550468262, lr [1.2999999999999999e-08]\nbatch 9 of epoch 1, loss 6.257152080535889, batch_acc 0.2116806937005866, lr [1.2999999999999999e-08]\nbatch 10 of epoch 1, loss 6.407853126525879, batch_acc 0.1956124314442413, lr [1.2999999999999999e-08]\nbatch 11 of epoch 1, loss 6.407754898071289, batch_acc 0.20710382513661202, lr [1.2999999999999999e-08]\nbatch 12 of epoch 1, loss 6.235937118530273, batch_acc 0.20949350304958897, lr [1.95e-08]\nbatch 13 of epoch 1, loss 6.281140327453613, batch_acc 0.19807983830217282, lr [1.95e-08]\nbatch 14 of epoch 1, loss 6.362402439117432, batch_acc 0.20120876353563333, lr [1.95e-08]\nbatch 15 of epoch 1, loss 6.082928657531738, batch_acc 0.21103979460847241, lr [1.95e-08]\nbatch 16 of epoch 1, loss 6.2607550621032715, batch_acc 0.22274229074889867, lr [2.5999999999999998e-08]\nbatch 17 of epoch 1, loss 6.145151138305664, batch_acc 0.2078740157480315, lr [2.5999999999999998e-08]\nbatch 18 of epoch 1, loss 6.162047386169434, batch_acc 0.2165991902834008, lr [2.5999999999999998e-08]\nbatch 19 of epoch 1, loss 6.371072292327881, batch_acc 0.2032937365010799, lr [2.5999999999999998e-08]\nbatch 20 of epoch 1, loss 6.570745944976807, batch_acc 0.19900359811790755, lr [3.25e-08]\nbatch 21 of epoch 1, loss 6.196685314178467, batch_acc 0.2109011229800055, lr [3.25e-08]\nbatch 22 of epoch 1, loss 6.235726833343506, batch_acc 0.21162603803911065, lr [3.25e-08]\nbatch 23 of epoch 1, loss 6.2447614669799805, batch_acc 0.20567375886524822, lr [3.25e-08]\nbatch 24 of epoch 1, loss 6.532299518585205, batch_acc 0.20797138477261115, lr [3.9e-08]\nbatch 25 of epoch 1, loss 6.225175380706787, batch_acc 0.21094552929085303, lr [3.9e-08]\nbatch 26 of epoch 1, loss 6.453901767730713, batch_acc 0.1962962962962963, lr [3.9e-08]\nbatch 27 of epoch 1, loss 6.41995906829834, batch_acc 0.20921450151057402, lr [3.9e-08]\nbatch 28 of epoch 1, loss 6.038833141326904, batch_acc 0.21818643637287274, lr [4.55e-08]\nbatch 29 of epoch 1, loss 6.327356815338135, batch_acc 0.2084412221646815, lr [4.55e-08]\nbatch 30 of epoch 1, loss 6.165412902832031, batch_acc 0.2182214948172395, lr [4.55e-08]\nbatch 31 of epoch 1, loss 6.439380168914795, batch_acc 0.20523797532107782, lr [4.55e-08]\nbatch 32 of epoch 1, loss 6.4269328117370605, batch_acc 0.19430933852140078, lr [5.1999999999999996e-08]\nbatch 33 of epoch 1, loss 6.254967212677002, batch_acc 0.21518310612135794, lr [5.1999999999999996e-08]\nbatch 34 of epoch 1, loss 6.260996341705322, batch_acc 0.21200850159404888, lr [5.1999999999999996e-08]\nbatch 35 of epoch 1, loss 6.329998016357422, batch_acc 0.21371896595853596, lr [5.1999999999999996e-08]\nbatch 36 of epoch 1, loss 6.335716724395752, batch_acc 0.20542338193911486, lr [5.8499999999999994e-08]\nbatch 37 of epoch 1, loss 6.102880001068115, batch_acc 0.21949286846275753, lr [5.8499999999999994e-08]\nbatch 38 of epoch 1, loss 6.211040496826172, batch_acc 0.21791370835608956, lr [5.8499999999999994e-08]\nbatch 39 of epoch 1, loss 6.268064022064209, batch_acc 0.2062062062062062, lr [5.8499999999999994e-08]\nbatch 40 of epoch 1, loss 6.314848899841309, batch_acc 0.21617293835068055, lr [6.5e-08]\nbatch 41 of epoch 1, loss 6.444400310516357, batch_acc 0.20764880150821438, lr [6.5e-08]\nbatch 42 of epoch 1, loss 6.547001838684082, batch_acc 0.20091693635382957, lr [6.5e-08]\nbatch 43 of epoch 1, loss 6.37795877456665, batch_acc 0.20613465037691708, lr [6.5e-08]\nbatch 44 of epoch 1, loss 6.0668625831604, batch_acc 0.22578859988931932, lr [7.149999999999999e-08]\nbatch 45 of epoch 1, loss 6.518086910247803, batch_acc 0.19989735694123684, lr [7.149999999999999e-08]\nbatch 46 of epoch 1, loss 6.145159721374512, batch_acc 0.21071044133476857, lr [7.149999999999999e-08]\nbatch 47 of epoch 1, loss 6.564383029937744, batch_acc 0.189873417721519, lr [7.149999999999999e-08]\nbatch 48 of epoch 1, loss 6.2038116455078125, batch_acc 0.20766932270916336, lr [7.8e-08]\nbatch 49 of epoch 1, loss 6.274891376495361, batch_acc 0.22478829869130101, lr [7.8e-08]\nbatch 50 of epoch 1, loss 6.409307479858398, batch_acc 0.20534093855327976, lr [7.8e-08]\nbatch 51 of epoch 1, loss 6.288203239440918, batch_acc 0.21486949644081202, lr [7.8e-08]\nbatch 52 of epoch 1, loss 6.111286163330078, batch_acc 0.21864272511222604, lr [8.45e-08]\nbatch 53 of epoch 1, loss 6.617668628692627, batch_acc 0.19856152067814026, lr [8.45e-08]\nbatch 54 of epoch 1, loss 6.110633373260498, batch_acc 0.21959009848283204, lr [8.45e-08]\nbatch 55 of epoch 1, loss 6.367786884307861, batch_acc 0.20615911035072712, lr [8.45e-08]\nbatch 56 of epoch 1, loss 6.313926696777344, batch_acc 0.21163490471414242, lr [9.1e-08]\nbatch 57 of epoch 1, loss 6.616863250732422, batch_acc 0.19070024910047054, lr [9.1e-08]\nbatch 58 of epoch 1, loss 6.563857078552246, batch_acc 0.19962536794219962, lr [9.1e-08]\nbatch 59 of epoch 1, loss 6.353222846984863, batch_acc 0.21187096774193548, lr [9.1e-08]\nbatch 60 of epoch 1, loss 6.519201755523682, batch_acc 0.20687802256851157, lr [9.749999999999999e-08]\nbatch 61 of epoch 1, loss 6.364423751831055, batch_acc 0.19867021276595745, lr [9.749999999999999e-08]\nbatch 62 of epoch 1, loss 6.050510406494141, batch_acc 0.22771023302938195, lr [9.749999999999999e-08]\nbatch 63 of epoch 1, loss 6.320257186889648, batch_acc 0.20363455314911624, lr [9.749999999999999e-08]\nbatch 64 of epoch 1, loss 6.311358451843262, batch_acc 0.20406410990269033, lr [1.0399999999999999e-07]\nbatch 65 of epoch 1, loss 6.181147575378418, batch_acc 0.21728455683771175, lr [1.0399999999999999e-07]\nbatch 66 of epoch 1, loss 6.494656562805176, batch_acc 0.19817232375979113, lr [1.0399999999999999e-07]\nbatch 67 of epoch 1, loss 6.288228511810303, batch_acc 0.21717029087828296, lr [1.0399999999999999e-07]\nbatch 68 of epoch 1, loss 6.242650985717773, batch_acc 0.20728744939271254, lr [1.105e-07]\nbatch 69 of epoch 1, loss 6.374406337738037, batch_acc 0.2135064935064935, lr [1.105e-07]\nbatch 70 of epoch 1, loss 6.346818923950195, batch_acc 0.21139755766621438, lr [1.105e-07]\nbatch 71 of epoch 1, loss 6.305731296539307, batch_acc 0.20174007612833061, lr [1.105e-07]\nbatch 72 of epoch 1, loss 6.461707592010498, batch_acc 0.19179929862422443, lr [1.1699999999999999e-07]\nbatch 73 of epoch 1, loss 6.286249160766602, batch_acc 0.20672313393329805, lr [1.1699999999999999e-07]\nbatch 74 of epoch 1, loss 6.478011131286621, batch_acc 0.20041109969167523, lr [1.1699999999999999e-07]\nbatch 75 of epoch 1, loss 6.339250564575195, batch_acc 0.21398416886543536, lr [1.1699999999999999e-07]\nbatch 76 of epoch 1, loss 6.340121746063232, batch_acc 0.1973340303188709, lr [1.235e-07]\nbatch 77 of epoch 1, loss 6.597214698791504, batch_acc 0.20518602029312288, lr [1.235e-07]\nbatch 78 of epoch 1, loss 6.348329544067383, batch_acc 0.20926474548882307, lr [1.235e-07]\nbatch 79 of epoch 1, loss 6.309140682220459, batch_acc 0.2025281670788678, lr [1.235e-07]\nbatch 80 of epoch 1, loss 6.329209804534912, batch_acc 0.20659226576693485, lr [1.3e-07]\nbatch 81 of epoch 1, loss 6.275179386138916, batch_acc 0.2026991954321308, lr [1.3e-07]\nbatch 82 of epoch 1, loss 6.343637943267822, batch_acc 0.208991169387209, lr [1.3e-07]\nbatch 83 of epoch 1, loss 6.109565734863281, batch_acc 0.22792892837312603, lr [1.3e-07]\nbatch 84 of epoch 1, loss 6.221691608428955, batch_acc 0.22172949002217296, lr [1.365e-07]\nbatch 85 of epoch 1, loss 6.353452205657959, batch_acc 0.21787277877929437, lr [1.365e-07]\nbatch 86 of epoch 1, loss 6.291635513305664, batch_acc 0.21210553543714433, lr [1.365e-07]\nbatch 87 of epoch 1, loss 6.243819236755371, batch_acc 0.2111801242236025, lr [1.365e-07]\nbatch 88 of epoch 1, loss 6.3435773849487305, batch_acc 0.21107178968655207, lr [1.4299999999999997e-07]\nbatch 89 of epoch 1, loss 6.162816524505615, batch_acc 0.2042850024912805, lr [1.4299999999999997e-07]\nbatch 90 of epoch 1, loss 6.208375930786133, batch_acc 0.2093773257256264, lr [1.4299999999999997e-07]\nbatch 91 of epoch 1, loss 6.516485214233398, batch_acc 0.19839015151515152, lr [1.4299999999999997e-07]\nbatch 92 of epoch 1, loss 6.256538391113281, batch_acc 0.21575082299316284, lr [1.4949999999999998e-07]\nbatch 93 of epoch 1, loss 6.391667366027832, batch_acc 0.20487682252388134, lr [1.4949999999999998e-07]\nbatch 94 of epoch 1, loss 6.350507736206055, batch_acc 0.19836289222373807, lr [1.4949999999999998e-07]\nbatch 95 of epoch 1, loss 6.508633136749268, batch_acc 0.19534767383691845, lr [1.4949999999999998e-07]\nbatch 96 of epoch 1, loss 6.324539661407471, batch_acc 0.2119309262166405, lr [1.56e-07]\nbatch 97 of epoch 1, loss 6.29547119140625, batch_acc 0.19560155239327295, lr [1.56e-07]\nbatch 98 of epoch 1, loss 6.262811660766602, batch_acc 0.21097154575749807, lr [1.56e-07]\nbatch 99 of epoch 1, loss 6.195268154144287, batch_acc 0.21930773391022174, lr [1.56e-07]\nbatch 100 of epoch 1, loss 6.367647647857666, batch_acc 0.19630606860158312, lr [1.625e-07]\nbatch 101 of epoch 1, loss 6.523090362548828, batch_acc 0.19169751454257006, lr [1.625e-07]\nbatch 102 of epoch 1, loss 6.224518775939941, batch_acc 0.21452236333517394, lr [1.625e-07]\nbatch 103 of epoch 1, loss 6.454647541046143, batch_acc 0.2090305444887118, lr [1.625e-07]\nbatch 104 of epoch 1, loss 6.267070293426514, batch_acc 0.21052631578947367, lr [1.69e-07]\nbatch 105 of epoch 1, loss 6.4647369384765625, batch_acc 0.2030995106035889, lr [1.69e-07]\nbatch 106 of epoch 1, loss 6.385537624359131, batch_acc 0.20486591097594692, lr [1.69e-07]\nbatch 107 of epoch 1, loss 6.19598913192749, batch_acc 0.2079134922585402, lr [1.69e-07]\nbatch 108 of epoch 1, loss 6.605029106140137, batch_acc 0.20309530274232962, lr [1.7549999999999998e-07]\nbatch 109 of epoch 1, loss 6.043338298797607, batch_acc 0.22175512348922755, lr [1.7549999999999998e-07]\nbatch 110 of epoch 1, loss 6.127506256103516, batch_acc 0.21550624840601887, lr [1.7549999999999998e-07]\nbatch 111 of epoch 1, loss 6.558412551879883, batch_acc 0.1981637337413925, lr [1.7549999999999998e-07]\nbatch 112 of epoch 1, loss 6.2427496910095215, batch_acc 0.21402394331785976, lr [1.82e-07]\nbatch 113 of epoch 1, loss 6.3246893882751465, batch_acc 0.21214619400934323, lr [1.82e-07]\nbatch 114 of epoch 1, loss 6.5657525062561035, batch_acc 0.20348232848232847, lr [1.82e-07]\nbatch 115 of epoch 1, loss 6.425989151000977, batch_acc 0.19497354497354497, lr [1.82e-07]\nbatch 116 of epoch 1, loss 6.453888416290283, batch_acc 0.20364583333333333, lr [1.885e-07]\nbatch 117 of epoch 1, loss 6.028347015380859, batch_acc 0.21800461656835085, lr [1.885e-07]\nbatch 118 of epoch 1, loss 6.592977523803711, batch_acc 0.2016282225237449, lr [1.885e-07]\nbatch 119 of epoch 1, loss 6.222174644470215, batch_acc 0.2143055170501802, lr [1.885e-07]\nbatch 120 of epoch 1, loss 6.117219924926758, batch_acc 0.2180701305349373, lr [1.9499999999999999e-07]\nbatch 121 of epoch 1, loss 5.951570987701416, batch_acc 0.22829028290282902, lr [1.9499999999999999e-07]\nbatch 122 of epoch 1, loss 6.398492813110352, batch_acc 0.2010856155933876, lr [1.9499999999999999e-07]\nbatch 123 of epoch 1, loss 6.487107276916504, batch_acc 0.20274636510500807, lr [1.9499999999999999e-07]\nbatch 124 of epoch 1, loss 6.456310749053955, batch_acc 0.19680446360639106, lr [2.015e-07]\nbatch 125 of epoch 1, loss 6.407958507537842, batch_acc 0.205685618729097, lr [2.015e-07]\nbatch 126 of epoch 1, loss 6.2700514793396, batch_acc 0.2100296575896468, lr [2.015e-07]\nbatch 127 of epoch 1, loss 6.260952949523926, batch_acc 0.21480096179535133, lr [2.015e-07]\nbatch 128 of epoch 1, loss 6.174307823181152, batch_acc 0.21593116847071883, lr [2.0799999999999998e-07]\nbatch 129 of epoch 1, loss 6.2861785888671875, batch_acc 0.20511440107671602, lr [2.0799999999999998e-07]\nbatch 130 of epoch 1, loss 6.179916858673096, batch_acc 0.21664943123061015, lr [2.0799999999999998e-07]\nbatch 131 of epoch 1, loss 6.2911763191223145, batch_acc 0.20607318972229433, lr [2.0799999999999998e-07]\nbatch 132 of epoch 1, loss 6.138885498046875, batch_acc 0.22545846817691478, lr [2.145e-07]\nbatch 133 of epoch 1, loss 6.235774517059326, batch_acc 0.21099067305268465, lr [2.145e-07]\nbatch 134 of epoch 1, loss 6.817010879516602, batch_acc 0.19405732316592164, lr [2.145e-07]\nbatch 135 of epoch 1, loss 6.533956527709961, batch_acc 0.2131885593220339, lr [2.145e-07]\nbatch 136 of epoch 1, loss 6.447052955627441, batch_acc 0.19779035300458098, lr [2.21e-07]\nbatch 137 of epoch 1, loss 6.494656562805176, batch_acc 0.20162133891213388, lr [2.21e-07]\nbatch 138 of epoch 1, loss 5.9456400871276855, batch_acc 0.22631578947368422, lr [2.21e-07]\nbatch 139 of epoch 1, loss 6.54961633682251, batch_acc 0.19462927756653992, lr [2.21e-07]\nbatch 140 of epoch 1, loss 6.371310234069824, batch_acc 0.20832177531206658, lr [2.2750000000000002e-07]\nbatch 141 of epoch 1, loss 6.1634674072265625, batch_acc 0.2166179984072206, lr [2.2750000000000002e-07]\nbatch 142 of epoch 1, loss 6.119292259216309, batch_acc 0.22570532915360503, lr [2.2750000000000002e-07]\nbatch 143 of epoch 1, loss 6.077171325683594, batch_acc 0.22146951537258988, lr [2.2750000000000002e-07]\nbatch 144 of epoch 1, loss 6.366231441497803, batch_acc 0.20721191533838515, lr [2.3399999999999998e-07]\nbatch 145 of epoch 1, loss 6.386909484863281, batch_acc 0.20280082987551867, lr [2.3399999999999998e-07]\nbatch 146 of epoch 1, loss 6.556493282318115, batch_acc 0.20681520314547838, lr [2.3399999999999998e-07]\nbatch 147 of epoch 1, loss 6.171029567718506, batch_acc 0.20854205114684946, lr [2.3399999999999998e-07]\nbatch 148 of epoch 1, loss 6.424921035766602, batch_acc 0.1964379256155055, lr [2.4049999999999996e-07]\nbatch 149 of epoch 1, loss 6.140849590301514, batch_acc 0.22257512826777426, lr [2.4049999999999996e-07]\nbatch 150 of epoch 1, loss 6.147756099700928, batch_acc 0.21004453240969817, lr [2.4049999999999996e-07]\nbatch 151 of epoch 1, loss 6.237802982330322, batch_acc 0.21793921423276502, lr [2.4049999999999996e-07]\nbatch 152 of epoch 1, loss 6.518186569213867, batch_acc 0.20808625336927225, lr [2.47e-07]\nbatch 153 of epoch 1, loss 6.410528182983398, batch_acc 0.2055190538764783, lr [2.47e-07]\nbatch 154 of epoch 1, loss 6.538980484008789, batch_acc 0.20089396573131363, lr [2.47e-07]\nbatch 155 of epoch 1, loss 6.059229373931885, batch_acc 0.22422394678492238, lr [2.47e-07]\nbatch 156 of epoch 1, loss 6.209424018859863, batch_acc 0.21633663366336633, lr [2.535e-07]\nbatch 157 of epoch 1, loss 6.4522786140441895, batch_acc 0.20010802052389953, lr [2.535e-07]\nbatch 158 of epoch 1, loss 6.484123229980469, batch_acc 0.1959385290889133, lr [2.535e-07]\nbatch 159 of epoch 1, loss 6.283641338348389, batch_acc 0.2145023435346016, lr [2.535e-07]\nbatch 160 of epoch 1, loss 6.28095006942749, batch_acc 0.20527494401592436, lr [2.6e-07]\nbatch 161 of epoch 1, loss 6.188057899475098, batch_acc 0.2068034557235421, lr [2.6e-07]\nbatch 162 of epoch 1, loss 6.232242107391357, batch_acc 0.21489413482599173, lr [2.6e-07]\nbatch 163 of epoch 1, loss 6.278139114379883, batch_acc 0.2146328293736501, lr [2.6e-07]\nbatch 164 of epoch 1, loss 6.401747226715088, batch_acc 0.20884909138793784, lr [2.665e-07]\nbatch 165 of epoch 1, loss 6.420486927032471, batch_acc 0.2042308696787673, lr [2.665e-07]\nbatch 166 of epoch 1, loss 6.294983386993408, batch_acc 0.21480331262939958, lr [2.665e-07]\nbatch 167 of epoch 1, loss 6.119909763336182, batch_acc 0.20524246395806028, lr [2.665e-07]\nbatch 168 of epoch 1, loss 6.357462406158447, batch_acc 0.2163243531608429, lr [2.73e-07]\nbatch 169 of epoch 1, loss 6.459649085998535, batch_acc 0.2055947854426942, lr [2.73e-07]\nbatch 170 of epoch 1, loss 6.295823097229004, batch_acc 0.2173657964212379, lr [2.73e-07]\nbatch 171 of epoch 1, loss 6.351602554321289, batch_acc 0.21575342465753425, lr [2.73e-07]\nbatch 172 of epoch 1, loss 6.021137714385986, batch_acc 0.2216611966675082, lr [2.795e-07]\nbatch 173 of epoch 1, loss 6.284401893615723, batch_acc 0.21354855781952897, lr [2.795e-07]\nbatch 174 of epoch 1, loss 6.156909465789795, batch_acc 0.2110042735042735, lr [2.795e-07]\nbatch 175 of epoch 1, loss 6.240795612335205, batch_acc 0.21005189838841848, lr [2.795e-07]\nbatch 176 of epoch 1, loss 6.263484954833984, batch_acc 0.20810880164228893, lr [2.8599999999999994e-07]\nbatch 177 of epoch 1, loss 6.179006576538086, batch_acc 0.21287379624936645, lr [2.8599999999999994e-07]\nbatch 178 of epoch 1, loss 6.128576278686523, batch_acc 0.21746790838157565, lr [2.8599999999999994e-07]\nbatch 179 of epoch 1, loss 6.277111530303955, batch_acc 0.19860356865787432, lr [2.8599999999999994e-07]\nbatch 180 of epoch 1, loss 6.11005163192749, batch_acc 0.21399730820995963, lr [2.9249999999999995e-07]\nbatch 181 of epoch 1, loss 6.291103839874268, batch_acc 0.2060931899641577, lr [2.9249999999999995e-07]\nbatch 182 of epoch 1, loss 6.285094261169434, batch_acc 0.21293375394321767, lr [2.9249999999999995e-07]\nbatch 183 of epoch 1, loss 6.123807430267334, batch_acc 0.22022413343758143, lr [2.9249999999999995e-07]\nbatch 184 of epoch 1, loss 6.160764694213867, batch_acc 0.20898697155011964, lr [2.9899999999999996e-07]\nbatch 185 of epoch 1, loss 6.378715515136719, batch_acc 0.19061352910330362, lr [2.9899999999999996e-07]\nbatch 186 of epoch 1, loss 6.450067043304443, batch_acc 0.21237767786299921, lr [2.9899999999999996e-07]\nbatch 187 of epoch 1, loss 6.1471757888793945, batch_acc 0.22101353144070046, lr [2.9899999999999996e-07]\nbatch 188 of epoch 1, loss 6.524561405181885, batch_acc 0.20518244315177156, lr [3.055e-07]\nbatch 189 of epoch 1, loss 6.411980628967285, batch_acc 0.2086981903093987, lr [3.055e-07]\nbatch 190 of epoch 1, loss 6.288566589355469, batch_acc 0.21281101111699313, lr [3.055e-07]\nbatch 191 of epoch 1, loss 6.363007545471191, batch_acc 0.21298560173865796, lr [3.055e-07]\nbatch 192 of epoch 1, loss 6.516690731048584, batch_acc 0.19748888307611823, lr [3.12e-07]\nbatch 193 of epoch 1, loss 6.351856231689453, batch_acc 0.195510310623858, lr [3.12e-07]\nbatch 194 of epoch 1, loss 6.57853364944458, batch_acc 0.20081653483031386, lr [3.12e-07]\nbatch 195 of epoch 1, loss 6.392850875854492, batch_acc 0.20021130480718435, lr [3.12e-07]\nbatch 196 of epoch 1, loss 6.456849098205566, batch_acc 0.1990632318501171, lr [3.185e-07]\nbatch 197 of epoch 1, loss 6.202274799346924, batch_acc 0.212241653418124, lr [3.185e-07]\nbatch 198 of epoch 1, loss 6.287705421447754, batch_acc 0.20374449339207049, lr [3.185e-07]\nbatch 199 of epoch 1, loss 6.056159973144531, batch_acc 0.21509240246406572, lr [3.185e-07]\nbatch 200 of epoch 1, loss 6.281889915466309, batch_acc 0.21765209940017138, lr [3.25e-07]\nbatch 201 of epoch 1, loss 6.193483829498291, batch_acc 0.2128663686040735, lr [3.25e-07]\nbatch 202 of epoch 1, loss 6.490915298461914, batch_acc 0.19695011630912382, lr [3.25e-07]\nbatch 203 of epoch 1, loss 6.264969825744629, batch_acc 0.21981424148606812, lr [3.25e-07]\nbatch 204 of epoch 1, loss 6.282370567321777, batch_acc 0.20850749938529628, lr [3.3149999999999997e-07]\nbatch 205 of epoch 1, loss 6.5165300369262695, batch_acc 0.21148536720044175, lr [3.3149999999999997e-07]\nbatch 206 of epoch 1, loss 6.346577167510986, batch_acc 0.2117554447651535, lr [3.3149999999999997e-07]\nbatch 207 of epoch 1, loss 6.088803768157959, batch_acc 0.2141235813366961, lr [3.3149999999999997e-07]\nbatch 208 of epoch 1, loss 6.3020710945129395, batch_acc 0.21270426989984187, lr [3.38e-07]\nbatch 209 of epoch 1, loss 6.37921142578125, batch_acc 0.2090032154340836, lr [3.38e-07]\nbatch 210 of epoch 1, loss 6.33964729309082, batch_acc 0.21187621968218567, lr [3.38e-07]\nbatch 211 of epoch 1, loss 6.103074073791504, batch_acc 0.2262420382165605, lr [3.38e-07]\nbatch 212 of epoch 1, loss 6.3604736328125, batch_acc 0.21202695697252463, lr [3.445e-07]\nbatch 213 of epoch 1, loss 6.226446628570557, batch_acc 0.20952628839146278, lr [3.445e-07]\nbatch 214 of epoch 1, loss 6.377191066741943, batch_acc 0.2099416828658706, lr [3.445e-07]\nbatch 215 of epoch 1, loss 6.297826766967773, batch_acc 0.21165338645418327, lr [3.445e-07]\nbatch 216 of epoch 1, loss 6.445901870727539, batch_acc 0.20415879017013233, lr [3.5099999999999995e-07]\nbatch 217 of epoch 1, loss 6.292809963226318, batch_acc 0.2115089514066496, lr [3.5099999999999995e-07]\nbatch 218 of epoch 1, loss 6.407809734344482, batch_acc 0.20304430551780375, lr [3.5099999999999995e-07]\nbatch 219 of epoch 1, loss 6.221603870391846, batch_acc 0.21500128435653737, lr [3.5099999999999995e-07]\nbatch 220 of epoch 1, loss 6.3746256828308105, batch_acc 0.2068698347107438, lr [3.5749999999999997e-07]\nbatch 221 of epoch 1, loss 6.526495933532715, batch_acc 0.19443748433976446, lr [3.5749999999999997e-07]\nbatch 222 of epoch 1, loss 6.024584770202637, batch_acc 0.23003374578177727, lr [3.5749999999999997e-07]\nbatch 223 of epoch 1, loss 6.41112756729126, batch_acc 0.202133062468258, lr [3.5749999999999997e-07]\nbatch 224 of epoch 1, loss 6.267998218536377, batch_acc 0.2108998732572877, lr [3.64e-07]\nbatch 225 of epoch 1, loss 5.992918491363525, batch_acc 0.21863260706235912, lr [3.64e-07]\nbatch 226 of epoch 1, loss 6.347687244415283, batch_acc 0.21259418729817006, lr [3.64e-07]\nbatch 227 of epoch 1, loss 6.204822540283203, batch_acc 0.20989803531459836, lr [3.64e-07]\nbatch 228 of epoch 1, loss 6.23618221282959, batch_acc 0.21376404494382023, lr [3.705e-07]\nbatch 229 of epoch 1, loss 6.300931930541992, batch_acc 0.21522530086761824, lr [3.705e-07]\nbatch 230 of epoch 1, loss 6.3055620193481445, batch_acc 0.2066772655007949, lr [3.705e-07]\nbatch 231 of epoch 1, loss 6.2727580070495605, batch_acc 0.20333680917622524, lr [3.705e-07]\nbatch 232 of epoch 1, loss 6.110820770263672, batch_acc 0.22145769622833844, lr [3.77e-07]\nbatch 233 of epoch 1, loss 6.2895307540893555, batch_acc 0.19994879672299026, lr [3.77e-07]\nbatch 234 of epoch 1, loss 6.404745578765869, batch_acc 0.20800859983875303, lr [3.77e-07]\nbatch 235 of epoch 1, loss 6.2219624519348145, batch_acc 0.20805043646944713, lr [3.77e-07]\nbatch 236 of epoch 1, loss 6.348931312561035, batch_acc 0.20815031722791605, lr [3.8349999999999996e-07]\nbatch 237 of epoch 1, loss 6.289365768432617, batch_acc 0.2008249548852797, lr [3.8349999999999996e-07]\nbatch 238 of epoch 1, loss 6.082484722137451, batch_acc 0.22824427480916032, lr [3.8349999999999996e-07]\nbatch 239 of epoch 1, loss 6.333093643188477, batch_acc 0.19446661346102687, lr [3.8349999999999996e-07]\nbatch 240 of epoch 1, loss 6.539303779602051, batch_acc 0.19661286054511776, lr [3.8999999999999997e-07]\nbatch 241 of epoch 1, loss 6.537505626678467, batch_acc 0.20499060906895628, lr [3.8999999999999997e-07]\nbatch 242 of epoch 1, loss 6.127813816070557, batch_acc 0.21226539783240814, lr [3.8999999999999997e-07]\nbatch 243 of epoch 1, loss 6.17449951171875, batch_acc 0.21906952965235174, lr [3.8999999999999997e-07]\nbatch 244 of epoch 1, loss 6.302868843078613, batch_acc 0.20147921448610048, lr [3.965e-07]\nbatch 245 of epoch 1, loss 6.432575225830078, batch_acc 0.1988748241912799, lr [3.965e-07]\nbatch 246 of epoch 1, loss 6.071706295013428, batch_acc 0.22345913657344557, lr [3.965e-07]\nbatch 247 of epoch 1, loss 6.14793062210083, batch_acc 0.21410057024364956, lr [3.965e-07]\nbatch 248 of epoch 1, loss 5.9521484375, batch_acc 0.2365005192107996, lr [4.03e-07]\nbatch 249 of epoch 1, loss 6.417796611785889, batch_acc 0.20424135319363798, lr [4.03e-07]\nbatch 250 of epoch 1, loss 6.248325824737549, batch_acc 0.2162305048902987, lr [4.03e-07]\nbatch 251 of epoch 1, loss 6.175006866455078, batch_acc 0.21016592046352384, lr [4.03e-07]\nbatch 252 of epoch 1, loss 6.112612247467041, batch_acc 0.21533316182145615, lr [4.0949999999999995e-07]\nbatch 253 of epoch 1, loss 6.435849666595459, batch_acc 0.206, lr [4.0949999999999995e-07]\nbatch 254 of epoch 1, loss 6.404238700866699, batch_acc 0.21154362416107383, lr [4.0949999999999995e-07]\nbatch 255 of epoch 1, loss 6.018083572387695, batch_acc 0.2242274412855377, lr [4.0949999999999995e-07]\nbatch 256 of epoch 1, loss 6.446830749511719, batch_acc 0.20694444444444443, lr [4.1599999999999997e-07]\nbatch 257 of epoch 1, loss 6.524349689483643, batch_acc 0.19132342188642457, lr [4.1599999999999997e-07]\nbatch 258 of epoch 1, loss 6.296560764312744, batch_acc 0.20719571319214086, lr [4.1599999999999997e-07]\nbatch 259 of epoch 1, loss 6.117770195007324, batch_acc 0.22046186895810957, lr [4.1599999999999997e-07]\nbatch 260 of epoch 1, loss 6.301893711090088, batch_acc 0.20786092214663643, lr [4.225e-07]\nbatch 261 of epoch 1, loss 6.225978374481201, batch_acc 0.2036444225560207, lr [4.225e-07]\nbatch 262 of epoch 1, loss 6.301429748535156, batch_acc 0.20524246395806028, lr [4.225e-07]\nbatch 263 of epoch 1, loss 6.378912448883057, batch_acc 0.19863187230808207, lr [4.225e-07]\nbatch 264 of epoch 1, loss 6.134111404418945, batch_acc 0.23044172432144758, lr [4.29e-07]\nbatch 265 of epoch 1, loss 5.895232677459717, batch_acc 0.21881287726358148, lr [4.29e-07]\nbatch 266 of epoch 1, loss 6.347883224487305, batch_acc 0.19398838090426876, lr [4.29e-07]\nbatch 267 of epoch 1, loss 6.399874687194824, batch_acc 0.2048629073978272, lr [4.29e-07]\nbatch 268 of epoch 1, loss 6.253917694091797, batch_acc 0.2163377466899825, lr [4.355e-07]\nbatch 269 of epoch 1, loss 6.209412097930908, batch_acc 0.20061334014822388, lr [4.355e-07]\nbatch 270 of epoch 1, loss 6.373821258544922, batch_acc 0.19925925925925925, lr [4.355e-07]\nbatch 271 of epoch 1, loss 6.178846836090088, batch_acc 0.22127659574468084, lr [4.355e-07]\nbatch 272 of epoch 1, loss 5.9586615562438965, batch_acc 0.23239632618981354, lr [4.42e-07]\nbatch 273 of epoch 1, loss 6.179983615875244, batch_acc 0.20918367346938777, lr [4.42e-07]\nbatch 274 of epoch 1, loss 6.380222320556641, batch_acc 0.1982851018220793, lr [4.42e-07]\nbatch 275 of epoch 1, loss 6.287529945373535, batch_acc 0.20899053627760253, lr [4.42e-07]\nbatch 276 of epoch 1, loss 6.283914089202881, batch_acc 0.21277705345501954, lr [4.485e-07]\nbatch 277 of epoch 1, loss 6.235342502593994, batch_acc 0.21610950250065808, lr [4.485e-07]\nbatch 278 of epoch 1, loss 6.100162506103516, batch_acc 0.21419420958237254, lr [4.485e-07]\nbatch 279 of epoch 1, loss 6.206840515136719, batch_acc 0.2101824121971141, lr [4.485e-07]\nbatch 280 of epoch 1, loss 6.2559309005737305, batch_acc 0.21925566343042072, lr [4.5500000000000004e-07]\nbatch 281 of epoch 1, loss 6.296239376068115, batch_acc 0.20962113659022932, lr [4.5500000000000004e-07]\nbatch 282 of epoch 1, loss 6.063265323638916, batch_acc 0.2180676039485492, lr [4.5500000000000004e-07]\nbatch 283 of epoch 1, loss 6.26000452041626, batch_acc 0.21522029372496662, lr [4.5500000000000004e-07]\nbatch 284 of epoch 1, loss 6.274792671203613, batch_acc 0.20925396412789185, lr [4.6149999999999994e-07]\nbatch 285 of epoch 1, loss 6.2208571434021, batch_acc 0.21568123393316196, lr [4.6149999999999994e-07]\nbatch 286 of epoch 1, loss 6.147600173950195, batch_acc 0.22183476394849785, lr [4.6149999999999994e-07]\nbatch 287 of epoch 1, loss 6.266654968261719, batch_acc 0.20408163265306123, lr [4.6149999999999994e-07]\nbatch 288 of epoch 1, loss 6.241984844207764, batch_acc 0.2047285009093271, lr [4.6799999999999996e-07]\nbatch 289 of epoch 1, loss 6.093855381011963, batch_acc 0.22065359477124183, lr [4.6799999999999996e-07]\nbatch 290 of epoch 1, loss 6.563756942749023, batch_acc 0.21006900076667517, lr [4.6799999999999996e-07]\nbatch 291 of epoch 1, loss 6.153568267822266, batch_acc 0.22459499263622976, lr [4.6799999999999996e-07]\nbatch 292 of epoch 1, loss 6.3137383460998535, batch_acc 0.21443089430894308, lr [4.7449999999999997e-07]\nbatch 293 of epoch 1, loss 6.389379978179932, batch_acc 0.20253807106598984, lr [4.7449999999999997e-07]\nbatch 294 of epoch 1, loss 6.1405744552612305, batch_acc 0.21331277306604987, lr [4.7449999999999997e-07]\nbatch 295 of epoch 1, loss 6.118292808532715, batch_acc 0.21315859855576358, lr [4.7449999999999997e-07]\nbatch 296 of epoch 1, loss 6.150065898895264, batch_acc 0.21160409556313994, lr [4.809999999999999e-07]\nbatch 297 of epoch 1, loss 6.29197883605957, batch_acc 0.21242697822623474, lr [4.809999999999999e-07]\nbatch 298 of epoch 1, loss 6.09637975692749, batch_acc 0.2162452931683701, lr [4.809999999999999e-07]\nbatch 299 of epoch 1, loss 6.3456597328186035, batch_acc 0.2017223382045929, lr [4.809999999999999e-07]\nbatch 300 of epoch 1, loss 6.531771659851074, batch_acc 0.2079624134520277, lr [4.875e-07]\nbatch 301 of epoch 1, loss 6.214417934417725, batch_acc 0.21641400940930475, lr [4.875e-07]\nbatch 302 of epoch 1, loss 6.227269172668457, batch_acc 0.20844327176781002, lr [4.875e-07]\nbatch 303 of epoch 1, loss 6.0391645431518555, batch_acc 0.22123430962343096, lr [4.875e-07]\nbatch 304 of epoch 1, loss 6.287505149841309, batch_acc 0.20786369593709042, lr [4.94e-07]\nbatch 305 of epoch 1, loss 6.242191791534424, batch_acc 0.220263223243109, lr [4.94e-07]\nbatch 306 of epoch 1, loss 6.020657062530518, batch_acc 0.2242943548387097, lr [4.94e-07]\nbatch 307 of epoch 1, loss 6.179638385772705, batch_acc 0.2030612244897959, lr [4.94e-07]\nbatch 308 of epoch 1, loss 6.229483604431152, batch_acc 0.21780528445608188, lr [5.005e-07]\nbatch 309 of epoch 1, loss 6.117787837982178, batch_acc 0.21649746192893402, lr [5.005e-07]\nbatch 310 of epoch 1, loss 6.415270805358887, batch_acc 0.19624270119319626, lr [5.005e-07]\nbatch 311 of epoch 1, loss 6.308182239532471, batch_acc 0.22011926367643245, lr [5.005e-07]\nbatch 312 of epoch 1, loss 6.373240947723389, batch_acc 0.20149068322981367, lr [5.07e-07]\nbatch 313 of epoch 1, loss 6.061785697937012, batch_acc 0.2219725343320849, lr [5.07e-07]\nbatch 314 of epoch 1, loss 6.105469226837158, batch_acc 0.22093023255813954, lr [5.07e-07]\nbatch 315 of epoch 1, loss 6.201888561248779, batch_acc 0.21470301850048684, lr [5.07e-07]\n","output_type":"stream"}]}]}